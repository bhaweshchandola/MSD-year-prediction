{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RNN  every node of the neural network is connected to itself many times which makes it possible to have a memory. But his causes the network to encounter a problem known as vanishing gradient problem.\n",
    "\n",
    "After forward propagation when the loss function is calculated and back propagation starts which updates the weights of the network it is done as while calculating the gradient of each node by its derivative w.r.t to weight and that is subtracted from it weight to reduce the loss function. In RNN since there are multiple nodes connected with one another in same layer this causes the gradient to vanish while reaching the last node and with this this vanishing gradient the learning rate of initial layers becomes very slow. To tackle this LSTM are used.\n",
    "\n",
    "Since during forward propagation weights are multiplied by the values and if the weight is small value becomes small while multiplying.\n",
    "Small Wrec (gradient weight) vanishing gradient\n",
    "Large Wrec exploding gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
