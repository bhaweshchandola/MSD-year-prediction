{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Part 1 - Building the CNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhawesh/.local/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Convolution\n",
    "# in this step feature detector is passed all over the image to create a feature map this is done multiple times\n",
    "# to create multiple feature maps\n",
    "# in ann dense was use but that is used in case of fully connected layer here that is not the case\n",
    "# 32 is number of filters i.e. number of feature maps\n",
    "# 3,3 is number of rows and columns of feature detector \n",
    "# niput shape is specified but it is done during datset processing\n",
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Pooling\n",
    "# in max pooling we basically reduce the size of feature map, we pass the 2*2 matrix over feature map\n",
    "# and select the max value from the matrix\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhawesh/.local/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Adding a second convolutional layer\n",
    "# this is added to increase the accuracy of the model we can either do this or add another fully connected layer\n",
    "# in a new convolution layer we don't need to specify the input shape as the input is being passed\n",
    "# through max pooling step\n",
    "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Flattening\n",
    "# during flattening we don't lose the features as during convolution we have extracted most of the features\n",
    "# so during flattening i.e. converting to 1-d array\n",
    "# we could've have directly flattened the image but this will not tell us the feature of the image\n",
    "# like it does after max pooling on other steps. Whereas orignal image just tells us the feature of the single \n",
    "# pixel which not very relevant compared to spatial feature extracted after convolution steps\n",
    "\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhawesh/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=128, activation=\"relu\")`\n",
      "  \n",
      "/home/bhawesh/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, activation=\"sigmoid\")`\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Full connection\n",
    "# now we need a fully connected layer to get the output in case of dog and cat classification\n",
    "# output dim is selected as 128 as it is good practice to select a number above 100\n",
    "# next activation is sigmoid since binary classification is present in case of multiple category softmax is used\n",
    "\n",
    "classifier.add(Dense(output_dim = 128, activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "'''\n",
    "image augmentation is applied here to avoid overfitting as during training this will augment data and apply\n",
    "some steps like rotating shearing etc\n",
    "target size is size mentioned in CNN\n",
    "class mode binay as only two classes present\n",
    "samples_per_epoch is number of images in training\n",
    "'''\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 89s 357ms/step - loss: 0.5381 - acc: 0.7245 - val_loss: 0.5079 - val_acc: 0.7540\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 87s 347ms/step - loss: 0.5137 - acc: 0.7483 - val_loss: 0.5973 - val_acc: 0.6935\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 86s 344ms/step - loss: 0.4833 - acc: 0.7651 - val_loss: 0.4923 - val_acc: 0.7645\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 83s 333ms/step - loss: 0.4722 - acc: 0.7735 - val_loss: 0.4740 - val_acc: 0.7815\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 81s 324ms/step - loss: 0.4442 - acc: 0.7933 - val_loss: 0.4816 - val_acc: 0.7690\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 81s 324ms/step - loss: 0.4503 - acc: 0.7877 - val_loss: 0.4598 - val_acc: 0.7920\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 81s 324ms/step - loss: 0.4216 - acc: 0.8005 - val_loss: 0.4677 - val_acc: 0.7950\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 81s 322ms/step - loss: 0.4123 - acc: 0.8089 - val_loss: 0.4684 - val_acc: 0.7930\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 80s 322ms/step - loss: 0.4010 - acc: 0.8158 - val_loss: 0.4608 - val_acc: 0.7875\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 85s 341ms/step - loss: 0.3938 - acc: 0.8235 - val_loss: 0.4501 - val_acc: 0.8055\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 90s 361ms/step - loss: 0.3777 - acc: 0.8301 - val_loss: 0.4411 - val_acc: 0.8040\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 96s 384ms/step - loss: 0.3727 - acc: 0.8326 - val_loss: 0.4745 - val_acc: 0.8010\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 98s 391ms/step - loss: 0.3568 - acc: 0.8409 - val_loss: 0.5337 - val_acc: 0.7910\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 119s 478ms/step - loss: 0.3432 - acc: 0.8508 - val_loss: 0.5177 - val_acc: 0.7720\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.3481 - acc: 0.8458 - val_loss: 0.4522 - val_acc: 0.8030\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 83s 333ms/step - loss: 0.3202 - acc: 0.8615 - val_loss: 0.5199 - val_acc: 0.7770\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 81s 324ms/step - loss: 0.3273 - acc: 0.8603 - val_loss: 0.4899 - val_acc: 0.7935\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 90s 359ms/step - loss: 0.3027 - acc: 0.8711 - val_loss: 0.4593 - val_acc: 0.8120\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 0.2908 - acc: 0.8731 - val_loss: 0.4936 - val_acc: 0.7940\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 85s 339ms/step - loss: 0.2810 - acc: 0.8783 - val_loss: 0.4987 - val_acc: 0.8070\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 82s 329ms/step - loss: 0.2757 - acc: 0.8809 - val_loss: 0.5253 - val_acc: 0.8070\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 94s 376ms/step - loss: 0.2537 - acc: 0.8882 - val_loss: 0.5096 - val_acc: 0.8100\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 93s 372ms/step - loss: 0.2559 - acc: 0.8911 - val_loss: 0.4899 - val_acc: 0.8125\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 90s 360ms/step - loss: 0.2461 - acc: 0.8957 - val_loss: 0.6086 - val_acc: 0.7820\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 83s 330ms/step - loss: 0.2414 - acc: 0.8990 - val_loss: 0.5441 - val_acc: 0.8005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb8334bc390>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier.fit_generator(training_set,\n",
    "#                          samples_per_epoch = 8000,\n",
    "#                          nb_epoch = 25,\n",
    "#                          validation_data = test_set,\n",
    "#                          nb_val_samples = 2000)\n",
    "\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch=8000/32,\n",
    "        epochs=25,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=2000/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "# for prediction\n",
    "\n",
    "from keras.preprocessing import image as image_utils\n",
    "import numpy as np\n",
    " \n",
    "test_image = image_utils.load_img('dataset/c1.jpeg', target_size=(64, 64))\n",
    "test_image = image_utils.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    " \n",
    "result = classifier.predict_on_batch(test_image)\n",
    "print(result) #dog=1 cat =0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = classifier.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "classifier.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
